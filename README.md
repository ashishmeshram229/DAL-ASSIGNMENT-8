# ðŸš€ Ensemble Learning: Biasâ€“Variance Trade-Off  

**Name:** Ashish Rajhans Meshram  
**Roll No:** DA25M016  
**Course:** DA5401 â€“ Machine Learning | IIT Madras  

This project analyzes how ensemble regression models â€” **Bagging**, **Boosting**, and **Stacking** â€” manage the **biasâ€“variance trade-off** using the **Bike Sharing Dataset (hour.csv)**.

---

## ðŸ§  Objective
- Load and preprocess data with One-Hot & Cyclical encoding.  
- Train baseline regressors (Decision Tree, Linear).  
- Implement ensemble techniques (**Bagging**, **Boosting**, **Stacking**).  
- Evaluate models using **RMSE** and visualize their biasâ€“variance behavior.

---

## âš™ï¸ Model Results

| Model | RMSE | Improvement |
|:--|:--:|:--|
| Decision Tree | 103.70 | Baseline |
| Bagging | 98.77 | â†“ Variance |
| Boosting | 81.49 | â†“ Bias |
| Stacking | 95.02 | â†“ Both |

**Best Performer:** Gradient Boosting Regressor  
**Most Balanced:** Stacking Regressor  

---

## ðŸ“Š Visual Insights
- **RMSE Comparison Chart** â€” performance ranking  
- **Residual Distribution** â€” bias and variance check  
- **Biasâ€“Variance Plot** â€” conceptual trade-off curve  
- **Learning Curve** â€” model generalization pattern  
- **Feature Importance (Boosting)** â€” dominant predictors  
- **Predicted vs Actual Scatter** â€” model alignment  

---

## ðŸ Conclusion
> Ensemble methods progressively reduce prediction error:
> - **Bagging** lowers variance  
> - **Boosting** lowers bias  
> - **Stacking** balances both  
>
> The **Gradient Boosting Regressor** achieved the lowest RMSE, validating the **biasâ€“variance trade-off principle** in ensemble learning.

